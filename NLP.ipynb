{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpNaaJA6C6lZPJ5Xivm/EV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineet1947/general_code-_files-/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the NLTK (Natural Language Toolkit) library here. "
      ],
      "metadata": {
        "id": "P5udL1l4jpew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "x6eOWXxJjAbf"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries\n",
        "import nltk\n",
        "import string\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We lowercase the text to reduce the size of the vocabulary of our text data."
      ],
      "metadata": {
        "id": "HspYY6a1jtN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_lowercase(text):\n",
        "\treturn text.lower()\n",
        "\n",
        "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
        "text_lowercase(input_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4S-kxzGHjMXY",
        "outputId": "0c9ea732-8ba8-47fd-ee7b-67beaebd215b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hey, did you know that the summer break is coming? amazing right !! it's only 5 more days !!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can either remove numbers or convert the numbers into their textual representations. \n",
        "We can use regular expressions to remove the numbers."
      ],
      "metadata": {
        "id": "JOmcx-m5juMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "\tresult = re.sub(r'\\d+', '', text)\n",
        "\treturn result\n",
        "\n",
        "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
        "remove_numbers(input_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LPf7BaeMjMat",
        "outputId": "e747a8ef-d7cc-4c16-a7cd-de614f6af1c3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are  balls in this bag, and  in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also convert the numbers into words. This can be done by using the inflect library."
      ],
      "metadata": {
        "id": "IWT5h0u0jw6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the inflect library\n",
        "import inflect\n",
        "p = inflect.engine()\n",
        "\n",
        "# convert number into words\n",
        "def convert_number(text):\n",
        "\t# split string into list of words\n",
        "\ttemp_str = text.split()\n",
        "\t# initialise empty list\n",
        "\tnew_string = []\n",
        "\n",
        "\tfor word in temp_str:\n",
        "\t\t# if word is a digit, convert the digit\n",
        "\t\t# to numbers and append into the new_string list\n",
        "\t\tif word.isdigit():\n",
        "\t\t\ttemp = p.number_to_words(word)\n",
        "\t\t\tnew_string.append(temp)\n",
        "\n",
        "\t\t# append the word as it is\n",
        "\t\telse:\n",
        "\t\t\tnew_string.append(word)\n",
        "\n",
        "\t# join the words of new_string to form a string\n",
        "\ttemp_str = ' '.join(new_string)\n",
        "\treturn temp_str\n",
        "\n",
        "input_str = 'There are 3 balls in this bag, and 12 in the other one.'\n",
        "convert_number(input_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_vUzLumGjMff",
        "outputId": "ed102b8e-df10-4762-e73c-bbb8286e6982"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are three balls in this bag, and twelve in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove punctuations so that we don’t have different forms of the same word. If we don’t remove the punctuation, then been. been, been! will be treated separately."
      ],
      "metadata": {
        "id": "wxU_gK6Cj0nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation\n",
        "def remove_punctuation(text):\n",
        "\ttranslator = str.maketrans('', '', string.punctuation)\n",
        "\treturn text.translate(translator)\n",
        "\n",
        "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
        "remove_punctuation(input_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nKKtJJacjMiD",
        "outputId": "d616b029-cd30-456c-9132-2708b883e70d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey did you know that the summer break is coming Amazing right  Its only 5 more days '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the join and split function to remove all the white spaces in a string."
      ],
      "metadata": {
        "id": "yIkU1T2vj3Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove whitespace from text\n",
        "def remove_whitespace(text):\n",
        "\treturn \" \".join(text.split())\n",
        "\n",
        "input_str = \" we don't need the given questions\"\n",
        "remove_whitespace(input_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c5byxTK4jMn1",
        "outputId": "cc58bc08-64a5-40e1-b38b-85afc2ddbd9e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"we don't need the given questions\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove default stopwords:\n",
        "\n",
        "Stopwords are words that do not contribute to the meaning of a sentence. Hence, they can safely be removed without causing any change in the meaning of the sentence. The NLTK library has a set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
      ],
      "metadata": {
        "id": "QDAGVb7Bjkk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "rLXcgpcRlJHQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DICij9iMlQPY",
        "outputId": "2a4f73c7-1dae-4dbe-ec34-c75aa6e5ea8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQnyPgy7lVYG",
        "outputId": "010f46d3-b602-47b8-e010-3fe8a5ac0e4a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# remove stopwords function\n",
        "def remove_stopwords(text):\n",
        "\tstop_words = set(stopwords.words(\"english\"))\n",
        "\tword_tokens = word_tokenize(text)\n",
        "\tfiltered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\treturn filtered_text\n",
        "\n",
        "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
        "remove_stopwords(example_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC7cHb_LjMs-",
        "outputId": "be5de3b1-e7c4-4722-8f4e-7cf4d5e00ea7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming:\n",
        "\n",
        "Stemming is the process of getting the root form of a word. Stem or root is the part to which inflectional affixes (-ed, -ize, -de, -s, etc.) are added. The stem of a word is created by removing the prefix or suffix of a word. So, stemming a word may not result in actual words.\n",
        "Example: \n",
        " \n",
        "\n",
        "books      --->    book\n",
        "looked     --->    look\n",
        "denied     --->    deni\n",
        "flies      --->    fli"
      ],
      "metadata": {
        "id": "S-YyAne6lg43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the text is not in tokens, then we need to convert it into tokens. After we have converted strings of text into tokens, we can convert the word tokens into their root form. There are mainly three algorithms for stemming. These are the Porter Stemmer, the Snowball Stemmer and the Lancaster Stemmer. Porter Stemmer is the most common among them."
      ],
      "metadata": {
        "id": "_xoVTAYQlk6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# stem words in the list of tokenized words\n",
        "def stem_words(text):\n",
        "\tword_tokens = word_tokenize(text)\n",
        "\tstems = [stemmer.stem(word) for word in word_tokens]\n",
        "\treturn stems\n",
        "\n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "stem_words(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HEESODOlgsC",
        "outputId": "8ac0991c-57e0-451f-c9ce-1141d345277d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'scienc',\n",
              " 'use',\n",
              " 'scientif',\n",
              " 'method',\n",
              " 'algorithm',\n",
              " 'and',\n",
              " 'mani',\n",
              " 'type',\n",
              " 'of',\n",
              " 'process']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization:\n",
        "\n",
        "Like stemming, lemmatization also converts a word to its root form. The only difference is that lemmatization ensures that the root word belongs to the language. We will get valid words if we use lemmatization. In NLTK, we use the WordNetLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization. So, we add the part-of-speech as a parameter. "
      ],
      "metadata": {
        "id": "dokvTJ1FlpDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfimLPYBlu1m",
        "outputId": "3476b671-f209-4b8c-e970-3dcc022ec037"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxElBBOlzMO",
        "outputId": "e1ddaead-44d4-47b0-896f-e3b2e1ec12a9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "\tword_tokens = word_tokenize(text)\n",
        "\t# provide context i.e. part-of-speech\n",
        "\tlemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "\treturn lemmas\n",
        "\n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "lemmatize_word(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bW1Cr_fjM1y",
        "outputId": "1949842a-b299-46c1-daa5-483b16e47722"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'science',\n",
              " 'use',\n",
              " 'scientific',\n",
              " 'methods',\n",
              " 'algorithms',\n",
              " 'and',\n",
              " 'many',\n",
              " 'type',\n",
              " 'of',\n",
              " 'process']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7y5PoOJSjM5K"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}